## Agent System Project Structure.

## ğŸ“‚ Project Structure

The project is organized to separate the UI (Streamlit), Data Layer, and Core Logic (Agent/Tools).

```text
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ Home.py              # ğŸ  Application Entry Point (Landing Page & Project Overview)
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ 1_Translate.py   # ğŸ—£ï¸ Interactive Mode: Human-AI Translation Interface
â”‚       â”œâ”€â”€ 2_Evaluate.py    # ğŸ“Š Evaluation Mode: Batch Testing, Ablation Studies (A-I) & Metrics
â”‚       â””â”€â”€ 3_Resources.py   # ğŸ“š Data Viewer: Inspect Monier-Williams Dict, Glossaries & History
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ mw_lexicon.db        # ğŸ“– DuckDB: Monier-Williams Dictionary Storage
â”‚   â”œâ”€â”€ history.db           # ğŸ•°ï¸ Database: Translation logs, user feedback, and agent traces
â”‚   â”œâ”€â”€ glossary/            # ğŸ“ Domain-specific constraints (CSV/JSONL)
â”‚   â”‚   â”œâ”€â”€ sanskrit_glossary.csv
â”‚   â”‚   â””â”€â”€ sanskrit_glossary.jsonl
â”‚   â””â”€â”€ testsets/            # ğŸ“‚ Parallel Corpora for Evaluation
â”‚       â”œâ”€â”€ mkb/             # Example dataset folder (e.g., Mahabharata)
â”‚       â””â”€â”€ ...
â”œâ”€â”€ models/                  # ğŸ¤– Local LLM Artifacts
â”‚   â”œâ”€â”€ Qwen2.5-7B-GGUF/Qwen2.5-7B-Instruct-Q4_K_M.gguf    # Quantized models 
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ orchestrator.py  # ğŸ§  Core Brain: Manages the Draft -> Tool -> Revise loop
â”‚   â”‚   â””â”€â”€ state.py         # ğŸ“¦ Data Class: Passes context (logs, drafts) between agent steps
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â””â”€â”€ duckdb_conn.py   # ğŸ”Œ Connection Manager: Handles Dictionary & History DB connections
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ prompts.py       # ğŸ’¬ Prompt Engineering: System instructions for Modes A-I
â”‚   â”‚   â””â”€â”€ qwen_local.py    # ğŸ”— LLM Wrapper: Interface for Ollama/Transformers
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ dict_lookup.py   # ğŸ” Tool: Monier-Williams Dictionary Retrieval
â”‚       â”œâ”€â”€ glossary_lookup.py # ğŸ” Tool: Glossary Constraint Enforcement
â”‚       â””â”€â”€ morph_lookup.py  # ğŸ§© Tool: Morphological Segmentation & Analysis
â”œâ”€â”€ requirements.txt         # ğŸ“¦ Python Dependencies
â””â”€â”€ README.md                # ğŸ“„ Project Documentation
```


## âœ… Prerequisites
- Python: 3.10 or 3.11

- Editor: VS Code (Recommended)

- Git: For cloning the repository

- (Optional) It is better if you also used Mac.

## ğŸš€ Quick Start Guide

Download and Open the Agentic_system project folder in VS Code, open the terminal (Ctrl + ~), and follow these steps.

### Step 1: Create & Activate Virtual Environment

Always use a virtual environment to keep dependencies isolated.

```text
# 1. Create venv
python3 -m venv venv

# 2. Activate venv
source venv/bin/activate
```
Windows (PowerShell)
```text
# 1. Create venv
python -m venv venv

# 2. Activate venv
.\venv\Scripts\activate
```

### Step 2: Install Dependencies (With Hardware Acceleration)

CRITICAL STEP: To enable GPU acceleration (Metal on Mac, CUDA on Windows), you must install llama-cpp-python with specific flags before installing the rest of the requirements.

#### ğŸ macOS (Apple Silicon M1/M2/M3) - Recommended

Enable Metal Performance Shaders (MPS):

```text
CMAKE_ARGS="-DGGML_METAL=on" pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir
```
#### ğŸªŸ Windows (NVIDIA GPU)
Pre-requisite: Install Build Tools for Visual Studio and CUDA Toolkit.

```text
$env:CMAKE_ARGS = "-DGGML_CUDA=on"; pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir
```

#### ğŸ§ Linux (NVIDIA GPU)
```text
CMAKE_ARGS="-DGGML_CUDA=on" pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir
```

#### ğŸŒ CPU Only (Universal fallback)
If you do not have a compatible GPU, simply run:
```text
pip install llama-cpp-python
```
After installing the core inference engine, install the rest:
```text
pip install -r requirements.txt
```

### Step 3: Download the GGUF Model
We use the 4-bit quantized version of Qwen2.5 to save memory (approx. 4.7GB) without sacrificing much performance.

```text
# Download the model to the local 'models' directory
huggingface-cli download bartowski/Qwen2.5-7B-Instruct-GGUF --include "Qwen2.5-7B-Instruct-Q4_K_M.gguf" --local-dir models/Qwen2.5-7B-GGUF --local-dir-use-symlinks False
```
Verification: Ensure the file exists at: models/Qwen2.5-7B-GGUF/Qwen2.5-7B-Instruct-Q4_K_M.gguf

### Step 4: Run the System
Launch the application using python -m streamlit to ensure path variables are handled correctly.

```text
python -m streamlit run app/Home.py
```

The application should automatically open in your browser at http://localhost:8501.


## ğŸ“‚ Data Management


### 1. Adding New Test Sets

To evaluate the system on new data (e.g., bible), structure your files as follows:

```text
data/
  â””â”€â”€ testsets/
      â””â”€â”€ bible/
          â”œâ”€â”€ bible.sa  (Sanskrit Source Lines)
          â””â”€â”€ bible.en  (English Reference Lines)
```
Go to the Evaluate page in the UI and click Scan & Ingest Local Datasets.

### 2. Dictionaries & Glossaries

Monier-Williams Dictionary: The system connects to the DuckDB database in data/ automatically.

Glossaries: Upload PDF, CSV, or JSONL glossaries via the Ingest tab in the UI to enforce terminology constraints.

## ğŸ›  Troubleshooting

### Q: ModuleNotFoundError: No module named 'llama_cpp'

A: You are likely not inside the virtual environment.

Ensure your terminal prompt shows (venv).

Run the app using python -m streamlit run ... instead of just streamlit run.

### Q: zsh: killed or Memory Overflow

A: Your system ran out of RAM.

Ensure you downloaded the GGUF (Quantized) version in Step 3, not the full 15GB model.

Close other memory-intensive applications (Chrome tabs, Docker, etc.).

### Q: xcrun: error: invalid active developer path (macOS)

A: You are missing Xcode command line tools required to compile llama-cpp. Run:

```text
xcode-select --install
```

### Q: Build fails on Windows

A: Installing llama-cpp-python on Windows can be tricky. You must have Visual Studio Community (with C++ development tools) installed. If CUDA fails, try the CPU-only installation command.



