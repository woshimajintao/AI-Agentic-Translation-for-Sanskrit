{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1Xj7sbTbYCo84EtQvm3NkZh0Wo90lDxNI",
      "authorship_tag": "ABX9TyO4b4CYHS0HF9Aw3B24nKmz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woshimajintao/AI-Agentic-Translation-for-Sanskrit/blob/main/gpt_eval/GPT_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Project root (repo root when on GitHub / local)\n",
        "PROJECT_ROOT = Path(\".\").resolve()\n",
        "\n",
        "# Data root\n",
        "DATA_ROOT = PROJECT_ROOT / \"data\"\n",
        "\n",
        "# Cache (already relative, keep it)\n",
        "CACHE_DIR = PROJECT_ROOT / \"gpt_cache\"\n",
        "CACHE_DIR.mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "_ejBio4doBMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U openai sacrebleu pandas tqdm gitpython \"httpx>=0.28.1,<1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUqtdYA8WR3w",
        "outputId": "a7b47243-f2a5-42bb-eab6-d47ea4c38fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m123.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def find_parallel_files(data_root: Path):\n",
        "    pairs = []\n",
        "    for dirpath, _, filenames in os.walk(data_root):\n",
        "        dirpath = Path(dirpath)\n",
        "        ens = {Path(f).stem: dirpath / f for f in filenames if f.endswith(\".en\")}\n",
        "        sas = {Path(f).stem: dirpath / f for f in filenames if f.endswith(\".sa\")}\n",
        "        for stem in sorted(set(ens) & set(sas)):\n",
        "            pairs.append({\n",
        "                \"folder\": str(dirpath.relative_to(data_root)),\n",
        "                \"stem\": stem,\n",
        "                \"en_path\": ens[stem],\n",
        "                \"sa_path\": sas[stem],\n",
        "            })\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "yhHSmC9rWd2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF\n",
        "\n",
        "bleu_metric = BLEU(tokenize=\"13a\")\n",
        "chrf2_metric = CHRF(word_order=2)\n",
        "\n",
        "def read_lines(path: Path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [ln.strip() for ln in f]\n",
        "    return [ln for ln in lines if ln]\n",
        "\n",
        "def eval_metrics(hyps, refs):\n",
        "    bleu = bleu_metric.corpus_score(hyps, [refs]).score\n",
        "    chrf2 = chrf2_metric.corpus_score(hyps, [refs]).score\n",
        "    return bleu, chrf2"
      ],
      "metadata": {
        "id": "VfRK4SplWgAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, time\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"gpt\")\n",
        "    except ImportError:\n",
        "        raise RuntimeError(\"Please set OPENAI_API_KEY as an environment variable.\")\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "MODEL = \"gpt-5\"\n",
        "TEMP = 0.2\n",
        "\n",
        "def cache_path(folder, stem):\n",
        "    safe = (folder.replace(\"/\", \"__\") + \"__\" + stem).replace(\"..\", \"_\")\n",
        "    return CACHE_DIR / f\"{safe}.jsonl\"\n",
        "\n",
        "def load_cache(path: Path):\n",
        "    cache = {}\n",
        "    if path.exists():\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                obj = json.loads(line)\n",
        "                cache[obj[\"i\"]] = obj[\"hyp\"]\n",
        "    return cache\n",
        "\n",
        "def append_cache(path: Path, i: int, hyp: str):\n",
        "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps({\"i\": i, \"hyp\": hyp}, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "def gpt_translate_one(sa_text: str) -> str:\n",
        "    prompt = (\n",
        "        \"Translate the following Sanskrit into natural English.\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"1) Keep proper names consistent.\\n\"\n",
        "        \"2) Do not add information not in the source.\\n\"\n",
        "        \"3) Output only the English translation.\\n\\n\"\n",
        "        f\"Sanskrit:\\n{sa_text}\\n\"\n",
        "    )\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a careful Sanskrit-to-English translator.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "ST3huqndWjNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Itihasa"
      ],
      "metadata": {
        "id": "SKow7sKusWzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "DOMAIN = \"itihasa\"\n",
        "SUBSET = \"testset_subset_42\"\n",
        "\n",
        "DOMAIN_DIR = DATA_ROOT / DOMAIN / SUBSET\n",
        "\n",
        "EN_PATH = DOMAIN_DIR / \"testset.en\"\n",
        "SA_PATH = DOMAIN_DIR / \"testset.sa\"\n",
        "\n",
        "assert EN_PATH.exists(), EN_PATH\n",
        "assert SA_PATH.exists(), SA_PATH\n",
        "\n",
        "MAX_N = None\n",
        "FOLDER_NAME = \"itihasa\"\n",
        "MODEL_TAG = \"gpt5\"\n",
        "STEM_NAME = f\"testset_subset_42_{MODEL_TAG}\"\n",
        "\n",
        "en_lines = read_lines(EN_PATH)\n",
        "sa_lines = read_lines(SA_PATH)\n",
        "\n",
        "n = min(len(en_lines), len(sa_lines))\n",
        "if MAX_N is not None:\n",
        "    n = min(n, MAX_N)\n",
        "\n",
        "refs = en_lines[:n]\n",
        "srcs = sa_lines[:n]\n",
        "\n",
        "print(f\"Loaded {n} sentence pairs\")\n",
        "\n",
        "cpath = cache_path(FOLDER_NAME, STEM_NAME)\n",
        "cache = load_cache(cpath)\n",
        "\n",
        "hyps = []\n",
        "\n",
        "for i in tqdm(range(n), desc=f\"{FOLDER_NAME}/{STEM_NAME} n={n}\"):\n",
        "    if i in cache:\n",
        "        hyp = cache[i]\n",
        "    else:\n",
        "        hyp = gpt_translate_one(srcs[i])\n",
        "        append_cache(cpath, i, hyp)\n",
        "        time.sleep(0.25)\n",
        "    hyps.append(hyp)\n",
        "\n",
        "bleu, chrf2 = eval_metrics(hyps, refs)\n",
        "\n",
        "print(\"\\nBLEU :\", round(bleu, 2))\n",
        "print(\"chrF2:\", round(chrf2, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqQSJZ28lG7M",
        "outputId": "98763ea3-0829-475e-abb0-e24cb77ff46e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 100 sentence pairs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "itihasa/testset_subset_42_gpt5 n=100: 100%|██████████| 100/100 [1:02:56<00:00, 37.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU : 7.41\n",
            "chrF2: 32.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bible"
      ],
      "metadata": {
        "id": "hw1H_NVxskGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOMAIN = \"bible\"\n",
        "SUBSET = \"testset_subset_42\"\n",
        "\n",
        "DOMAIN_DIR = DATA_ROOT / DOMAIN / SUBSET\n",
        "\n",
        "EN_PATH = DOMAIN_DIR / \"testset.en\"\n",
        "SA_PATH = DOMAIN_DIR / \"testset.sa\"\n",
        "\n",
        "assert EN_PATH.exists(), EN_PATH\n",
        "assert SA_PATH.exists(), SA_PATH\n",
        "\n",
        "MAX_N = None\n",
        "FOLDER_NAME = \"bible\"\n",
        "MODEL_TAG = \"gpt-5\"\n",
        "STEM_NAME = f\"testset_subset_42_{MODEL_TAG}\"\n",
        "\n",
        "en_lines = read_lines(EN_PATH)\n",
        "sa_lines = read_lines(SA_PATH)\n",
        "\n",
        "n = min(len(en_lines), len(sa_lines))\n",
        "if MAX_N is not None:\n",
        "    n = min(n, MAX_N)\n",
        "\n",
        "refs = en_lines[:n]\n",
        "srcs = sa_lines[:n]\n",
        "\n",
        "print(f\"Loaded {n} sentence pairs\")\n",
        "\n",
        "cpath = cache_path(FOLDER_NAME, STEM_NAME)\n",
        "cache = load_cache(cpath)\n",
        "\n",
        "hyps = []\n",
        "\n",
        "for i in tqdm(range(n), desc=f\"{FOLDER_NAME}/{STEM_NAME} n={n}\"):\n",
        "    if i in cache:\n",
        "        hyp = cache[i]\n",
        "    else:\n",
        "        hyp = gpt_translate_one(srcs[i])\n",
        "        append_cache(cpath, i, hyp)\n",
        "        time.sleep(0.25)\n",
        "    hyps.append(hyp)\n",
        "\n",
        "bleu, chrf2 = eval_metrics(hyps, refs)\n",
        "\n",
        "print(\"\\nBLEU :\", round(bleu, 2))\n",
        "print(\"chrF2:\", round(chrf2, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdXclvJdq2Qf",
        "outputId": "6788ac47-cff5-4c90-f83c-45701fa7a1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 sentence pairs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "bible/testset_subset_42_gpt5 n=100: 100%|██████████| 100/100 [58:20<00:00, 35.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU : 16.48\n",
            "chrF2: 40.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gitasopanam"
      ],
      "metadata": {
        "id": "1lwJFU2K6YYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOMAIN = \"gitasopanam\"\n",
        "SUBSET = \"testset_subset_42\"\n",
        "\n",
        "DOMAIN_DIR = DATA_ROOT / DOMAIN / SUBSET\n",
        "\n",
        "EN_PATH = GITASOPANAM_DIR / \"testset.en\"\n",
        "SA_PATH = GITASOPANAM_DIR / \"testset.sa\"\n",
        "\n",
        "assert EN_PATH.exists(), EN_PATH\n",
        "assert SA_PATH.exists(), SA_PATH\n",
        "\n",
        "MAX_N = None\n",
        "FOLDER_NAME = \"gitasopanam\"\n",
        "MODEL_TAG = \"gpt5\"\n",
        "STEM_NAME = f\"testset_subset_42_{MODEL_TAG}\"\n",
        "\n",
        "en_lines = read_lines(EN_PATH)\n",
        "sa_lines = read_lines(SA_PATH)\n",
        "\n",
        "n = min(len(en_lines), len(sa_lines))\n",
        "if MAX_N is not None:\n",
        "    n = min(n, MAX_N)\n",
        "\n",
        "refs = en_lines[:n]\n",
        "srcs = sa_lines[:n]\n",
        "\n",
        "print(f\"Loaded {n} sentence pairs\")\n",
        "\n",
        "cpath = cache_path(FOLDER_NAME, STEM_NAME)\n",
        "cache = load_cache(cpath)\n",
        "\n",
        "hyps = []\n",
        "\n",
        "for i in tqdm(range(n), desc=f\"{FOLDER_NAME}/{STEM_NAME} n={n}\"):\n",
        "    if i in cache:\n",
        "        hyp = cache[i]\n",
        "    else:\n",
        "        hyp = gpt_translate_one(srcs[i])\n",
        "        append_cache(cpath, i, hyp)\n",
        "        time.sleep(0.25)\n",
        "    hyps.append(hyp)\n",
        "\n",
        "bleu, chrf2 = eval_metrics(hyps, refs)\n",
        "\n",
        "print(\"\\nBLEU :\", round(bleu, 2))\n",
        "print(\"chrF2:\", round(chrf2, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OYLvoL20elX",
        "outputId": "2b866d38-207a-4da4-b6e6-9d567803f467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 sentence pairs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "gitasopanam/testset_subset_42_gpt5 n=100: 100%|██████████| 100/100 [23:57<00:00, 14.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU : 22.91\n",
            "chrF2: 48.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mkb"
      ],
      "metadata": {
        "id": "ZXoiht1b7IIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOMAIN = \"mkb\"\n",
        "SUBSET = \"testset_subset_42\"\n",
        "\n",
        "DOMAIN_DIR = DATA_ROOT / DOMAIN / SUBSET\n",
        "\n",
        "EN_PATH = DOMAIN_DIR / \"testset.en\"\n",
        "SA_PATH = DOMAIN_DIR / \"testset.sa\"\n",
        "\n",
        "assert EN_PATH.exists(), EN_PATH\n",
        "assert SA_PATH.exists(), SA_PATH\n",
        "\n",
        "MAX_N = None\n",
        "FOLDER_NAME = \"mkb\"\n",
        "MODEL_TAG = \"gpt5\"\n",
        "STEM_NAME = f\"testset_subset_42_{MODEL_TAG}\"\n",
        "\n",
        "en_lines = read_lines(EN_PATH)\n",
        "sa_lines = read_lines(SA_PATH)\n",
        "\n",
        "n = min(len(en_lines), len(sa_lines))\n",
        "if MAX_N is not None:\n",
        "    n = min(n, MAX_N)\n",
        "\n",
        "refs = en_lines[:n]\n",
        "srcs = sa_lines[:n]\n",
        "\n",
        "print(f\"Loaded {n} sentence pairs\")\n",
        "\n",
        "cpath = cache_path(FOLDER_NAME, STEM_NAME)\n",
        "cache = load_cache(cpath)\n",
        "\n",
        "hyps = []\n",
        "\n",
        "for i in tqdm(range(n), desc=f\"{FOLDER_NAME}/{STEM_NAME} n={n}\"):\n",
        "    if i in cache:\n",
        "        hyp = cache[i]\n",
        "    else:\n",
        "        hyp = gpt_translate_one(srcs[i])\n",
        "        append_cache(cpath, i, hyp)\n",
        "        time.sleep(0.25)\n",
        "    hyps.append(hyp)\n",
        "\n",
        "bleu, chrf2 = eval_metrics(hyps, refs)\n",
        "\n",
        "print(\"\\nBLEU :\", round(bleu, 2))\n",
        "print(\"chrF2:\", round(chrf2, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGLoLppB7Hx_",
        "outputId": "56c2e19e-79de-48d5-b8b3-81e83c8323db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 sentence pairs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mkb/testset_subset_42_gpt5 n=100: 100%|██████████| 100/100 [35:00<00:00, 21.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU : 14.0\n",
            "chrF2: 40.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spoken-tutorials"
      ],
      "metadata": {
        "id": "T7Jh0EK-87fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOMAIN = \"spoken-tutorials\"\n",
        "SUBSET = \"testset_subset_42\"\n",
        "\n",
        "DOMAIN_DIR = DATA_ROOT / DOMAIN / SUBSET\n",
        "\n",
        "EN_PATH = DOMAIN_DIR / \"testset.en\"\n",
        "SA_PATH = DOMAIN_DIR / \"testset.sa\"\n",
        "\n",
        "assert EN_PATH.exists(), EN_PATH\n",
        "assert SA_PATH.exists(), SA_PATH\n",
        "\n",
        "MAX_N = None\n",
        "FOLDER_NAME = \"spoken-tutorials\"\n",
        "MODEL_TAG = \"gpt5\"\n",
        "STEM_NAME = f\"testset_subset_42_{MODEL_TAG}\"\n",
        "\n",
        "en_lines = read_lines(EN_PATH)\n",
        "sa_lines = read_lines(SA_PATH)\n",
        "\n",
        "n = min(len(en_lines), len(sa_lines))\n",
        "if MAX_N is not None:\n",
        "    n = min(n, MAX_N)\n",
        "\n",
        "refs = en_lines[:n]\n",
        "srcs = sa_lines[:n]\n",
        "\n",
        "print(f\"Loaded {n} sentence pairs\")\n",
        "\n",
        "cpath = cache_path(FOLDER_NAME, STEM_NAME)\n",
        "cache = load_cache(cpath)\n",
        "\n",
        "hyps = []\n",
        "\n",
        "for i in tqdm(range(n), desc=f\"{FOLDER_NAME}/{STEM_NAME} n={n}\"):\n",
        "    if i in cache:\n",
        "        hyp = cache[i]\n",
        "    else:\n",
        "        hyp = gpt_translate_one(srcs[i])\n",
        "        append_cache(cpath, i, hyp)\n",
        "        time.sleep(0.25)\n",
        "    hyps.append(hyp)\n",
        "\n",
        "bleu, chrf2 = eval_metrics(hyps, refs)\n",
        "\n",
        "print(\"\\nBLEU :\", round(bleu, 2))\n",
        "print(\"chrF2:\", round(chrf2, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33gHeLdJ9a1g",
        "outputId": "a3f1e7d6-54a3-475d-ad47-974876bbc818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 sentence pairs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "spoken-tutorials/testset_subset_42_gpt5 n=100: 100%|██████████| 100/100 [36:01<00:00, 21.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU : 22.08\n",
            "chrF2: 50.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}