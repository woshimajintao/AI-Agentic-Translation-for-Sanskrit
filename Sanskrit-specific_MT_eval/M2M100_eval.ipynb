{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install / upgrade dependencies (Colab)\n",
        "!pip -q install -U transformers sentencepiece sacrebleu accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziMGpqzT3iR7",
        "outputId": "bfb67627-bfaa-4600-9d58-754663822bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we used M2M100 models:\n",
        "https://huggingface.co/docs/transformers/en/model_doc/m2m_100"
      ],
      "metadata": {
        "id": "oG-S2_ANg9gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# =========================================================\n",
        "# Block 1) Paths: just replace with your dataset's .sa / .en\n",
        "# =========================================================\n",
        "# For the 5 datasets, you only need to change these two paths\n",
        "# to the corresponding <dataset>.sa and <dataset>.en files.\n",
        "SA_PATH = \"/content/testset.sa\"  # Sanskrit (source)\n",
        "EN_PATH = \"/content/testset.en\"  # English (reference)\n",
        "\n",
        "For each dataset, you only need to replace SA_PATH and EN_PATH with that dataset’s corresponding .sa and .en files (everything else stays the same).\n",
        "\n",
        "# =========================================================\n",
        "# Block 2) Read aligned parallel data (skips empty lines)\n",
        "# =========================================================\n",
        "def read_parallel(sa_path, en_path, n=None):\n",
        "    with open(sa_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        sa_lines = [ln.strip() for ln in f]\n",
        "    with open(en_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        en_lines = [ln.strip() for ln in f]\n",
        "\n",
        "    assert len(sa_lines) == len(en_lines), (\n",
        "        f\"Line count mismatch: sa={len(sa_lines)} en={len(en_lines)}\"\n",
        "    )\n",
        "\n",
        "    pairs = []\n",
        "    for s, e in zip(sa_lines, en_lines):\n",
        "        # skip if either side is empty\n",
        "        if not s or not e:\n",
        "            continue\n",
        "        pairs.append((s, e))\n",
        "        if n is not None and len(pairs) >= n:\n",
        "            break\n",
        "\n",
        "    if n is not None and len(pairs) < n:\n",
        "        print(f\"Warning: only got {len(pairs)} non-empty aligned pairs (requested {n}).\")\n",
        "\n",
        "    src_texts = [s for s, _ in pairs]  # Sanskrit\n",
        "    ref_texts = [e for _, e in pairs]  # English\n",
        "    return src_texts, ref_texts\n",
        "\n",
        "\n",
        "# n=None => full set; if you want only first 200 pairs, set n=200\n",
        "src_texts, ref_texts = read_parallel(SA_PATH, EN_PATH, n=None)\n",
        "print(\"Loaded non-empty aligned pairs:\", len(src_texts))\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Block 3) Load model + tokenizer\n",
        "# =========================================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "m2m_model_name = \"Swamitucats/M2M100_Sanskrit_English\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(m2m_model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(m2m_model_name).to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt_qaziqMkvR",
        "outputId": "52cbb38c-7359-4e0f-e3d6-46cef7b133ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded non-empty aligned pairs: 100\n",
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "M2M100ForConditionalGeneration(\n",
              "  (model): M2M100Model(\n",
              "    (shared): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
              "    (encoder): M2M100Encoder(\n",
              "      (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
              "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x M2M100EncoderLayer(\n",
              "          (self_attn): M2M100Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): ReLU()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): M2M100Decoder(\n",
              "      (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
              "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x M2M100DecoderLayer(\n",
              "          (self_attn): M2M100Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): M2M100Attention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=128112, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Block 4) Inference (keeps your original logic)\n",
        "# =========================================================\n",
        "def run_m2m_sanskrit_english_on_test(batch_size=16, max_length=256, max_examples=None):\n",
        "    preds = []\n",
        "    n = len(src_texts) if max_examples is None else min(len(src_texts), max_examples)\n",
        "\n",
        "    for i in range(0, n, batch_size):\n",
        "        batch = src_texts[i : min(i + batch_size, n)]  # Sanskrit (Devanagari)\n",
        "\n",
        "        enc = tokenizer(\n",
        "            batch,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(\n",
        "                **enc,\n",
        "                max_length=max_length,\n",
        "                num_beams=4,  # set to 1 for faster decoding\n",
        "            )\n",
        "\n",
        "        outputs = tokenizer.batch_decode(\n",
        "            gen,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=True,\n",
        "        )\n",
        "\n",
        "        assert len(outputs) == len(batch)\n",
        "        preds.extend(outputs)\n",
        "\n",
        "    assert len(preds) == n, f\"M2M preds length {len(preds)} != expected {n}\"\n",
        "    return preds\n",
        "\n"
      ],
      "metadata": {
        "id": "kD7VhqYZgZ7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Block 3) Load model + tokenizer\n",
        "# =========================================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "m2m_model_name = \"Swamitucats/M2M100_Sanskrit_English\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(m2m_model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(m2m_model_name).to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Block 4) Inference (keeps your original logic)\n",
        "# =========================================================\n",
        "def run_m2m_sanskrit_english_on_test(batch_size=16, max_length=256, max_examples=None):\n",
        "    preds = []\n",
        "    n = len(src_texts) if max_examples is None else min(len(src_texts), max_examples)\n",
        "\n",
        "    for i in range(0, n, batch_size):\n",
        "        batch = src_texts[i : min(i + batch_size, n)]  # Sanskrit (Devanagari)\n",
        "\n",
        "        enc = tokenizer(\n",
        "            batch,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(\n",
        "                **enc,\n",
        "                max_length=max_length,\n",
        "                num_beams=4,  # set to 1 for faster decoding\n",
        "            )\n",
        "\n",
        "        outputs = tokenizer.batch_decode(\n",
        "            gen,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=True,\n",
        "        )\n",
        "\n",
        "        assert len(outputs) == len(batch)\n",
        "        preds.extend(outputs)\n",
        "\n",
        "    assert len(preds) == n, f\"M2M preds length {len(preds)} != expected {n}\"\n",
        "    return preds\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Block 5) Run full set + evaluate + print examples\n",
        "# =========================================================\n",
        "m2m_max_examples = None  # None => run all; or set e.g. 200 / 1000\n",
        "m2m_preds = run_m2m_sanskrit_english_on_test(\n",
        "    batch_size=16,\n",
        "    max_length=256,\n",
        "    max_examples=m2m_max_examples,\n",
        ")\n",
        "\n",
        "eval_n = len(m2m_preds)\n",
        "_ = evaluate_mt(\n",
        "    f\"Baseline — M2M100_Sanskrit_English (Itihasa {eval_n} lines)\",\n",
        "    m2m_preds,\n",
        "    ref_texts[:eval_n],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgnbF0UWMh07",
        "outputId": "1f0a6705-4e1e-4a24-9714-e114b8e3fed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Baseline — M2M100_Sanskrit_English (gitasopanam 100 lines)\n",
            "  #pairs = 100\n",
            "  BLEU  = 6.79\n",
            "  chrF2 = 31.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a few samples\n",
        "for i in range(min(10, len(src_texts), len(m2m_preds))):\n",
        "    print(\"==== sample\", i, \"====\")\n",
        "    print(\"SRC:\", src_texts[i])\n",
        "    print(\"REF:\", ref_texts[i])\n",
        "    print(\"HYP:\", m2m_preds[i])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyKgG-FXMgro",
        "outputId": "49cb6a42-c1ee-4e61-d036-a0a66fe2542f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== sample 0 ====\n",
            "SRC: अहं अतिथिम् स्वागतं करोमि ।\n",
            "REF: I will welcome the guest.\n",
            "HYP: I do welcome my guest.\n",
            "\n",
            "==== sample 1 ====\n",
            "SRC: गुरुवासरः कदा भविष्यति ?\n",
            "REF: When will it be Thursday?\n",
            "HYP: When will he be born as a preceptor?\n",
            "\n",
            "==== sample 2 ====\n",
            "SRC: बालिका कन्दुकेन क्रीडितवती ।\n",
            "REF: Girl played with the ball.\n",
            "HYP: The maiden was sporting with the mace.\n",
            "\n",
            "==== sample 3 ====\n",
            "SRC: भवन्तः कौन्तेयाः ।\n",
            "REF: You all are sons of Kunti.\n",
            "HYP: You are the sons of Kunti.\n",
            "\n",
            "==== sample 4 ====\n",
            "SRC: चम्वाः नायकः धृष्टद्युम्नः ।\n",
            "REF: The leader of Chamva is Dhrishtadyumna.\n",
            "HYP: The Chambas are the heroes and Dhrishtadyumna.\n",
            "\n",
            "==== sample 5 ====\n",
            "SRC: रामः वनवासं समाप्य प्रत्यागच्छति ।\n",
            "REF: Rama returns concluding the exile to forest.\n",
            "HYP: Rāma having finished his abode in the forest, returns.\n",
            "\n",
            "==== sample 6 ====\n",
            "SRC: पूर्वं युयुत्सुः कौरवपक्षीयः आसीत् ।\n",
            "REF: Earlier Yuyutsu was on Kaurava side.\n",
            "HYP: In the days of yore Yuyutsu was the son of the Kuru race.\n",
            "\n",
            "==== sample 7 ====\n",
            "SRC: माता पुनः प्रक्षालयति ।\n",
            "REF: Mother again cleans.\n",
            "HYP: My mother again and again began to weep aloud.\n",
            "\n",
            "==== sample 8 ====\n",
            "SRC: सारथिः रथं चालयति।\n",
            "REF: Charioteer drives the chariot.\n",
            "HYP: The charioteer is driving the car.\n",
            "\n",
            "==== sample 9 ====\n",
            "SRC: प्राचार्यः छात्राणां कृते सम्भाषते ।\n",
            "REF: The Principal speaks to the students.\n",
            "HYP: The preceptor is speaking of the conduct of the pupils.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}