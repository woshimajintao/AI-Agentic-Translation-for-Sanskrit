{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install / upgrade dependencies (Colab)\n",
        "!pip -q install -U transformers sentencepiece sacrebleu accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziMGpqzT3iR7",
        "outputId": "bfb67627-bfaa-4600-9d58-754663822bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we used IndicTrans2 models:\n",
        "https://huggingface.co/ai4bharat/indictrans2-indic-en-1B"
      ],
      "metadata": {
        "id": "KFPIghN3h5Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from IndicTransToolkit.processor import IndicProcessor\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Block 1) Paths: just replace with your dataset's .sa / .en\n",
        "# =========================================================\n",
        "# For the 5 datasets, you only need to change these two paths\n",
        "# to the corresponding <dataset>.sa and <dataset>.en files.\n",
        "SA_PATH = \"/content/testset.sa\"  # Sanskrit (source)\n",
        "EN_PATH = \"/content/testset.en\"  # English (reference)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Block 2) Read aligned parallel data (skips empty lines)\n",
        "# =========================================================\n",
        "def read_parallel(sa_path, en_path, n=None):\n",
        "    with open(sa_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        sa_lines = [ln.strip() for ln in f]\n",
        "    with open(en_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        en_lines = [ln.strip() for ln in f]\n",
        "\n",
        "    assert len(sa_lines) == len(en_lines), (\n",
        "        f\"Line count mismatch: sa={len(sa_lines)} en={len(en_lines)}\"\n",
        "    )\n",
        "\n",
        "    pairs = []\n",
        "    for s, e in zip(sa_lines, en_lines):\n",
        "        if not s or not e:\n",
        "            continue\n",
        "        pairs.append((s, e))\n",
        "        if n is not None and len(pairs) >= n:\n",
        "            break\n",
        "\n",
        "    src_texts = [s for s, _ in pairs]  # Sanskrit (san_Deva)\n",
        "    ref_texts = [e for _, e in pairs]  # English (eng_Latn)\n",
        "    return src_texts, ref_texts\n",
        "\n",
        "\n",
        "# n=None => full set; if you want only first 200 pairs, set n=200\n",
        "src_texts, ref_texts = read_parallel(SA_PATH, EN_PATH, n=None)\n",
        "print(\"Loaded non-empty aligned pairs:\", len(src_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mhtzcjKh8ll",
        "outputId": "1ef86e6c-630a-4da4-9e64-879cf973b52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded non-empty aligned pairs: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Block 3) Load IndicTrans2 model (official from_pretrained)\n",
        "# =========================================================\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from IndicTransToolkit.processor import IndicProcessor\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "MODEL_NAME = \"ai4bharat/indictrans2-indic-en-1B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    dtype=\"auto\",  # auto-select fp16 / bf16 / fp32 based on hardware\n",
        "    # If you installed flash_attn and want faster decoding, you can add:\n",
        "    # attn_implementation=\"flash_attention_2\",\n",
        ").to(DEVICE).eval()\n",
        "\n",
        "ip = IndicProcessor(inference=True)"
      ],
      "metadata": {
        "id": "QroSw-9MiIGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Block 4) Inference (IndicProcessor preprocess + postprocess)\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def indictrans_translate_sa2en(\n",
        "    texts,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=128,\n",
        "    num_beams=1,\n",
        "):\n",
        "    src_lang, tgt_lang = \"san_Deva\", \"eng_Latn\"\n",
        "    outs = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i : i + batch_size]\n",
        "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            batch,\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True,\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        generated = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_beams=num_beams,\n",
        "            do_sample=False,\n",
        "            use_cache=False,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=3,\n",
        "            repetition_penalty=1.2,\n",
        "        )\n",
        "\n",
        "        decoded = tokenizer.batch_decode(\n",
        "            generated,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=True,\n",
        "        )\n",
        "        decoded = ip.postprocess_batch(decoded, lang=tgt_lang)\n",
        "        outs.extend(decoded)\n",
        "\n",
        "    return outs\n",
        "\n",
        "\n",
        "sa2en_preds = indictrans_translate_sa2en(\n",
        "    src_texts,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=128,\n",
        "    num_beams=4,\n",
        ")\n",
        "print(\"Generated:\", len(sa2en_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzlX87GwiIXn",
        "outputId": "ee9d00ed-9a05-44ca-af23-f98378f2bc29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Block 5) Evaluate (clean refs, then evaluate)\n",
        "# =========================================================\n",
        "import re\n",
        "\n",
        "def clean_ref(s: str) -> str:\n",
        "    s = re.sub(r\"\\*.*$\", \"\", s)  # remove trailing comments after '*'\n",
        "    return \" \".join(s.split())\n",
        "\n",
        "ref_clean = [clean_ref(x) for x in ref_texts]\n",
        "\n",
        "_ = evaluate_mt(\n",
        "    f\"IndicTrans2 (refs cleaned) — san_Deva→eng_Latn (testset {len(sa2en_preds)} lines)\",\n",
        "    sa2en_preds,\n",
        "    ref_clean[: len(sa2en_preds)],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdXhloMXiHj3",
        "outputId": "c42c493b-5d62-4e03-8a8d-9b57b5b3fd73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== IndicTrans2 (refs cleaned) — san_Deva→eng_Latn (testset 100 lines) =====\n",
            "Count: 100\n",
            "SacreBLEU: 13.03\n",
            "chrF++   : 38.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Block 6) Print a few samples\n",
        "# =========================================================\n",
        "for i in range(min(10, len(src_texts), len(sa2en_preds))):\n",
        "    print(\"==== sample\", i, \"====\")\n",
        "    print(\"SRC:\", src_texts[i])\n",
        "    print(\"REF:\", ref_texts[i])\n",
        "    print(\"HYP:\", sa2en_preds[i])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8G8CWJPtiFjt",
        "outputId": "60b7d8f8-52fb-4efb-d0aa-cb3017f6f317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== sample 0 ====\n",
            "SRC: अहं अतिथिम् स्वागतं करोमि ।\n",
            "REF: I will welcome the guest.\n",
            "HYP: I welcome the guest.\n",
            "\n",
            "==== sample 1 ====\n",
            "SRC: गुरुवासरः कदा भविष्यति ?\n",
            "REF: When will it be Thursday?\n",
            "HYP: when will it be thursday\n",
            "\n",
            "==== sample 2 ====\n",
            "SRC: बालिका कन्दुकेन क्रीडितवती ।\n",
            "REF: Girl played with the ball.\n",
            "HYP: The girl plays with a ball.\n",
            "\n",
            "==== sample 3 ====\n",
            "SRC: भवन्तः कौन्तेयाः ।\n",
            "REF: You all are sons of Kunti.\n",
            "HYP: You're a coward.\n",
            "\n",
            "==== sample 4 ====\n",
            "SRC: चम्वाः नायकः धृष्टद्युम्नः ।\n",
            "REF: The leader of Chamva is Dhrishtadyumna.\n",
            "HYP: Chamva is the hero of Dhrishtadyumna.\n",
            "\n",
            "==== sample 5 ====\n",
            "SRC: रामः वनवासं समाप्य प्रत्यागच्छति ।\n",
            "REF: Rama returns concluding the exile to forest.\n",
            "HYP: Rama returns after completing his exile.\n",
            "\n",
            "==== sample 6 ====\n",
            "SRC: पूर्वं युयुत्सुः कौरवपक्षीयः आसीत् ।\n",
            "REF: Earlier Yuyutsu was on Kaurava side.\n",
            "HYP: In the past, Yuyutsu was pro-Kaurava.\n",
            "\n",
            "==== sample 7 ====\n",
            "SRC: माता पुनः प्रक्षालयति ।\n",
            "REF: Mother again cleans.\n",
            "HYP: The mother washes again.\n",
            "\n",
            "==== sample 8 ====\n",
            "SRC: सारथिः रथं चालयति।\n",
            "REF: Charioteer drives the chariot.\n",
            "HYP: The charioteer drives the chariot.\n",
            "\n",
            "==== sample 9 ====\n",
            "SRC: प्राचार्यः छात्राणां कृते सम्भाषते ।\n",
            "REF: The Principal speaks to the students.\n",
            "HYP: The principal speaks for the students.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}